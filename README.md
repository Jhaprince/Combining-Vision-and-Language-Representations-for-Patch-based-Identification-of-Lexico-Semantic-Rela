# Combining Vision And Language Representations for Patch Based Identification Of Lexico Semantic Relations (ACMMM 2022)

This is the official repository accompanying the ACMMM 2022 full paper [Combining Vision And Language Representations for Patch Based Identification Of Lexico Semantic Relations]().  This repository contains codebase, dataset and the annotation guidelines.

# Authors
Prince Jha, Gael Dias, Alexis Lechervy, Jose G. Moreno, Anubhav Jangra, Sebastio Pais, Sriparna Saha 

# Dataset
1. [Text Data](https://drive.google.com/file/d/1wzlwbPbMiqVi7ODk5bSio7Pk3Xxcyi6x/view?usp=sharing)
2. [Image Data](),We have scrapped the images corresponding to each instance in the text data using BING API. We are still working on the method to open source this data, because of some license issues with the BING API. Maybe, in future we can upload the encoding of images or the urls.

# References
I would encourage to read following papers within the given sequence before moving to the code part
1. [Learning Lexical-Semantic Relations Using Intuitive Cognitive Links](https://dias.users.greyc.fr/publications/ecir2019.pdf)
2. [Patch-based Identification of Lexical Semantic Relations](https://dias.users.greyc.fr/publications/ecir2020.pdf)
3. [GloVe](https://nlp.stanford.edu/pubs/glove.pdf)
3. [CLIP](https://arxiv.org/pdf/2103.00020.pdf)
4. [The Verbal and Non Verbal Signals of Depression--Combining Acoustics, Text and Visuals for Estimating Depression Level](https://arxiv.org/pdf/1904.07656.pdf)
5. [CentralNet: a Multilayer Approach for Multimodal Fusion](https://arxiv.org/pdf/1808.07275.pdf)

# Code 
We have borrowed our code from following sources, you can also go visit these
1. [CLIP Feature Extractor](https://github.com/openai/CLIP)
2. [centralnet code](https://github.com/jperezrua/mfas)


# Citation
If you find this repository to be helpful please cite us

```
To be Done
```

